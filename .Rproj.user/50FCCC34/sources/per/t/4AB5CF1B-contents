---
title: "Recrutiment Task Solution"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE}
source("VarSummary.R")
library(tidyverse)
library(mice)
library(ROCit)
library(mlr3)
library(mlr3learners)
library(mlr3viz)
library(xgboost)
library(forcats)
library(forecast)
library(DALEX)
library(DALEXtra)
library(breakDown)

dataset <- read.csv("data.csv")
```
Variable Summary
```{r}
VarSummary(dataset)

```

As we can see, statemb has a 13,6% of NA variables, moreover NA is the most common value of this variable 

```{r}
df_no_mv <- dataset %>% drop_na()
densityplot(df_no_mv$statemb)
md.pattern(dataset)
imputed_data <- mice(dataset, m=5, maxit = 50, method = 'pmm', seed = 500)
densityplot(imputed_data)
```

Predictive mean matching as imputation method seems to work correct, the denisty lines are quite close to each other

```{r}
imputed_data$imp$statemb
dataset <- complete(imputed_data,3)
VarSummary(dataset)
```

```{r}
library(ggplot2)
ggplot(data=dataset,
       aes(x=rr, y=statemb)) +
       geom_point()

ggplot(data=dataset,
       aes(x=joblost, y=rr)) +
       geom_point()

ggplot(data=dataset,
       aes(x=dkids, y=joblost)) +
       geom_count()

print(addmargins(table(data.frame(dataset$dkids,dataset$joblost))))

```

Very little data about seasonal_job_ended and position_abolished, therefore i'm converting joblost to 2 categories

```{r}
joblost_categoories <- fct_lump(dataset$joblost, 2)
dataset$joblost <- joblost_categoories
```

```{r}
densityplot(dataset$statemb)
```
Get rid off fat tail

```{r}
lambda1 <- BoxCox.lambda(dataset$statemb)
dataset$statemb <- BoxCox(dataset$statemb, lambda1)

densityplot(dataset$statemb)

lambda2 <- BoxCox.lambda(dataset$stateur)
dataset$stateur <- BoxCox(dataset$stateur, lambda2)

densityplot(dataset$stateur)

```
```{r}
categorization <- function(selected_data) {
  qantiles <-
    quantile(selected_data, c(.2, .4, .6, .8), na.rm = TRUE)
  
  vector <- c()
  
  
  for (i in 1:length(selected_data)) {
    if (!is.na(selected_data[i])) {
      if (selected_data[i] <= qantiles[[1]]) {
        vector[i] <- 2
      }
      else if (selected_data[i] > qantiles[[1]] &&
               selected_data[i] <= qantiles[[2]]) {
        vector[i] <- 3
      }
      else if (selected_data[i] > qantiles[[2]] &&
               selected_data[i] <= qantiles[[3]]) {
        vector[i] <- 4
      }
      else if (selected_data[i] > qantiles[[3]] &&
               selected_data[i] <= qantiles[[4]]) {
        vector[i] <- 5
      }
      else if (selected_data[i] > qantiles[[4]]) {
        vector[i] <- 6
      }
      
    } else {
      vector[i] <- 1
    }
  }
  
  return(vector)
}

dataset$age <- as.factor(categorization(dataset$age))
dataset$statemb <- as.factor(categorization(dataset$statemb))
dataset$stateur <- as.factor(categorization(dataset$stateur))
```


```{r}
ggplot(data=dataset,
       aes(x=statemb, y=rr)) +
       geom_point()

dataset$state <- as.character(dataset$state)
ggplot(data=dataset,
       aes(x=state, y=rr)) +
       geom_point()
```
some of residence code has much less observations, we can also see that some of them differ in terms of distribution, for example

```{r}
densityplot(dataset$rr[dataset$state == "93"])
densityplot(dataset$rr[dataset$state == "22"])
```

State of residence code is categorical variable, therefore we could convert 51 unique values to 51 indicator variables. However it would mean that we would have too few degrees of freedom because we have only 5477 observations. 


```{r}
uds <- unique(dataset$state)

rr_length <- c()
for(i in 1:length(uds)){
  rr_length[i] <- length(dataset$rr[dataset$state == uds[i]])
}
rr_length

mm_measure <- c()
var_measure <- c()
for(i in 1:length(uds)){
  mm_measure[i] <- mean(dataset$rr[dataset$state == uds[i]]) + median(dataset$rr[dataset$state == uds[i]]) / 2
  var_measure[i] <- var(dataset$rr[dataset$state == uds[i]])
}
data.frame(mm_measure,var_measure)
```

```{r}
quantile(var_measure, probs = c(0.33, 0.66))

quantile(mm_measure, probs = c(0.33, 0.66))

quantile(mm_measure, probs = c(0.33, 0.66))

```

```{r}
state_categories <- fct_lump(dataset$state, 12)
table(state_categories)
```
^ Not balanced data

I am dropping the state variable because there is not enough data to perform encoding using patterns on other variables and the data describing the state are avaible in the statemb and stateur variables

```{r}

td <- cbind(
  rr = dataset$rr,
  data.frame(model.matrix(~ age - 1, dataset)),
  data.frame(model.matrix(~ stateur - 1, dataset)),
  data.frame(model.matrix(~ statemb - 1, dataset)),
  data.frame(model.matrix(~ joblost - 1, dataset)),
  data.frame(model.matrix(~ school12 - 1, dataset)),
  data.frame(model.matrix(~ married - 1, dataset)),
  data.frame(model.matrix(~ dkids - 1, dataset)),
  data.frame(model.matrix(~ dykids - 1, dataset)),
  data.frame(model.matrix(~ head - 1, dataset))
)


head(td)
```


```{r}

task <- TaskRegr$new(id = "rr", backend = td, target = "rr")

set.seed(123)
indicative_rmse <- function(task,learner_name){ 
  train_set = sample(task$nrow, 0.7 * task$nrow)
  test_set = setdiff(seq_len(task$nrow), train_set)
  learner = mlr_learners$get(learner_name)
  learner$train(task, row_ids = train_set)
  prediction = learner$predict(task, row_ids = test_set)
  measure <- msr("regr.rmse")
  return(prediction$score(measure))}
```

```{r}
indicative_rmse(task,"regr.cv_glmnet")
indicative_rmse(task,"regr.glmnet")
indicative_rmse(task,"regr.ranger")
indicative_rmse(task,"regr.rpart")
indicative_rmse(task,"regr.kknn")
indicative_rmse(task,"regr.xgboost")
indicative_rmse(task,"regr.featureless")
indicative_rmse(task,"regr.svm")

```

```{r}
train_data <- td[2:length(td)]
train_labels <- td$rr


cv <- xgb.cv(data = as.matrix(train_data), 
             label = train_labels,
             nrounds = 1000,
             nfold = 5,
             objective = "reg:linear",
             eta = 0.2,
             max_depth = 6,
             verbose = 0
)

elog <- cv$evaluation_log
elog %>% 
  summarize(ntrees.train = which.min(train_rmse_mean),  
            ntrees.test  = which.min(test_rmse_mean))  

model_xgb <- xgboost(data = as.matrix(train_data), 
                          label = td$rr,  
                          nrounds = 50,    
                          objective = "reg:linear",
                          eta = 0.2,
                          depth = 6,
                          verbose = 0
)

pred <- predict(model_xgb, as.matrix(train_data))
result <- data.frame(rr=td$rr,pred=pred)

ggplot(result, aes(x = pred, y = rr)) + 
  geom_point() + 
  geom_abline()

cat_result <- data.frame(rr=as.numeric(td$rr > 0.5),pred=as.numeric(pred > 0.5))

ROCit_obj <- rocit(cat_result$pred, cat_result$rr)
plot(ROCit_obj)

```

```{r}
explainer<- explain_xgboost(model_xgb,
                 data = as.matrix(train_data),  y = train_labels)

bd <- break_down(explainer,
                 as.matrix(train_data[1,]),
                 keep_distributions = TRUE)

plot(bd)
```
High intercept indicates that the model is weak

```{r}
source("TransformPredictData.R")


pred <- as.numeric(predict(model_xgb, as.matrix(hd)) > 0.5)

write.csv(pred,"data_predict_outcome.csv") 
```

```{r}
td$rr > 0.5
```